Over the past few days, I worked on designing and deploying a real-time data engineering pipeline â€” starting from a dummy API to storing structured data in an Amazon S3 bucket. Hereâ€™s a quick breakdown of what I accomplished:

ğŸ”¹ Step-by-Step Journey:

ğŸ“¦ Designed a FastAPI Microservice to simulate eCommerce order data on-demand.
âš™ï¸ Built a Kafka Producer to extract data from the API and publish it to a Kafka topic.
ğŸ“¥ Created a Kafka Consumer that listens and processes the real-time stream.
ğŸ” Streamed data with Apache Spark, consuming from Kafka and applying transformations.
â˜ï¸ Stored clean data into AWS S3, organizing it as structured .json files.
ğŸ› ï¸ Working to automate the entire workflow with Apache Airflow (Docker integration next!).

ğŸ›¤ï¸ This mini-project helped me solidify my concepts of:

Event streaming
Stream processing
Cloud storage integration
Real-world ETL pipeline design

ğŸ‘‡ Hereâ€™s a visual flow of the architecture:

<img width="991" alt="Screenshot 2025-06-16 at 9 26 28â€¯PM" src="https://github.com/user-attachments/assets/ee3cba86-bdca-4b60-9408-aebe2cd0e49e" />

ğŸ’¡ Technologies Used: Kafka Â· FastAPI Â· Apache Spark Â· Amazon S3 Â· Python Â· Airflow


